{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "superior-socket",
   "metadata": {},
   "source": [
    "# Вариант 1\n",
    "### О задании\n",
    "\n",
    "В этом задании мы будем применять метод k-ближайших соседей (kNN) для классификации изображений. Идея в том, чтобы сравнить различные векторные представления изображений между собой. Будем работать с одним из самых распространенных датасетов с картинками &mdash; CIFAR10. В качестве метрики качества выберем accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import CIFAR10\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CIFAR10('CIFAR10', train=True, download=True)\n",
    "test_set = CIFAR10('CIFAR10', train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-method",
   "metadata": {},
   "source": [
    "## 1. Исходные изображения (1 балл)\n",
    "\n",
    "Начнем с того, что попробуем использовать сами изображения в качестве признаковых описаний (то есть вытягиваем картинки в длинный вектор). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_set.data.reshape(50000, -1)\n",
    "y_train = np.array(train_set.targets)\n",
    "\n",
    "X_test = test_set.data.reshape(10000, -1)\n",
    "y_test = np.array(test_set.targets)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-stream",
   "metadata": {},
   "source": [
    "Обучите метод k-ближайших соседей из `sklearn` на картинках как на длинных векторах, подберите оптимальное число соседей по кросс-валидации. Насколько качество получилось лучше, чем у случайного прогноза?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-circumstances",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE ᕕ( ᐛ )ᕗ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-desktop",
   "metadata": {},
   "source": [
    "## 2. Предобученный классификатор (2.5 балл)\n",
    "\n",
    "Теперь попробуем использовать сверточную нейросеть, предобученную на ImageNet. Выберите архитектуру из `torchvision`, которая вам по душе, и используйте выход предпоследнего слоя как признаковое описание картинки. Проще всего будет найти поле класса модели, которое отвечает за последний слой (классификационную голову) и заменить его на болванку `nn.Identity` (у вас должно получиться что-то вроде `model.classifier = nn.Identity()`). В зависимости от архитектуры название этого поля может меняться, это можно уточнить в исходниках `torchvision` (смотрите документацию `torchvision.models`).\n",
    "\n",
    "Посчитайте эмбеддинги для обучающих и тестовых картинок, не забудьте использовать правильную трансформацию данных для ImageNet, а также перевести модель в `eval` режим. Обучите kNN для получившихся представлений (все так же с подбором гиперпараметра $k$). Прокомментируйте полученные результаты. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-rocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE ᕕ( ᐛ )ᕗ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-world",
   "metadata": {},
   "source": [
    "## 3. PCA (1.5 балла)\n",
    "\n",
    "Теперь будем пробовать разные методы для понижения размерности. Начнем с простейшего &mdash; обучите PCA из `sklearn` и используйте его выход в качестве признаков для kNN. Ради чистоты эксперимента возьмите число компонент равным размерности представлений из прошлого пункта. Не забываем про подбор гиперпараметра :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE ᕕ( ᐛ )ᕗ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-season",
   "metadata": {},
   "source": [
    "## 4. Автокодировщик (5 баллов)\n",
    "\n",
    "Теперь обратимся к нелинейным методам понижения размерности: будем экспериментировать с автокодировщиками. Обучите две вариации модели: полносвязную и сверточную. Первая будет работать с изображениями как с длинными векторами, вторая &mdash; как полагается. Рекомендуется пользоваться шаблонами, приведенными ниже (но если они уж очень вам не нравится, то это не обязательно). Как и прежде, возьмите размерность скрытого слоя равной размеру представлений из предобученного классификатора. Обязательно рисуйте графики лосса при обучении.\n",
    "\n",
    "Сравните, как восстанавливают изображения два автокодировщика (для этого зафиксируйте несколько тестовых картинок и нарисуйте их восстановленные версии). Сделайте выводы.\n",
    "\n",
    "Теперь по нашему стандартному протоколу: обучаем kNN, подбираем параметр числа соседей и смотрим на качество. Получилось ли качество лучше, чем при линейном понижении размерности? А чем при использовании предобученного классификатора? Попытайтесь объяснить, почему так происходит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-chamber",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    '''\n",
    "    Base autoencoder class\n",
    "    '''        \n",
    "    def encode(self, x):\n",
    "        '''\n",
    "        Encode batch of images\n",
    "        param x: batch of input images\n",
    "        return z: batch of latent vectors\n",
    "        '''\n",
    "        # YOUR CODE HERE ᕕ( ᐛ )ᕗ\n",
    "        pass\n",
    "    \n",
    "    def decode(self, z):\n",
    "        '''\n",
    "        Decode batch of latent vectors\n",
    "        param z: batch of latent vectors\n",
    "        return x_hat: batch of reconstructed images\n",
    "        '''\n",
    "        # YOUR CODE HERE ᕕ( ᐛ )ᕗ\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass of the autoencoder\n",
    "        param x: batch of input images\n",
    "        return x_hat: batch of reconstructed images\n",
    "        '''\n",
    "        # YOUR CODE HERE ᕕ( ᐛ )ᕗ\n",
    "        pass\n",
    "\n",
    "\n",
    "class FCAutoencoder(Autoencoder):\n",
    "    '''\n",
    "    Fully-connected autoencoder architecture\n",
    "    '''\n",
    "    def __init__(self, n_components):\n",
    "        '''\n",
    "        param n_components: dim of latent vectors\n",
    "        '''\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE ᕕ( ᐛ )ᕗ\n",
    "        pass\n",
    "\n",
    "\n",
    "class CNNAutoencoder(Autoencoder):\n",
    "    '''\n",
    "    Convolutional autoencoder architecture\n",
    "    '''\n",
    "    def __init__(self, n_components):\n",
    "        '''\n",
    "        param n_components: dim of latent vectors\n",
    "        '''\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE ᕕ( ᐛ )ᕗ\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE ᕕ( ᐛ )ᕗ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
